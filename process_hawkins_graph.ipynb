{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries (if not installed)\n",
    "%pip install plotly pandas ipywidgets\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import DatePicker, Dropdown, Output, HBox, interact, Layout\n",
    "\n",
    "# Ensure Plotly renders properly in Jupyter Notebook\n",
    "pio.renderers.default = \"jupyterlab\"  # or \"colab\" or \"iframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "zip_folder = \"csv_files/\"  # Where ZIP files are located\n",
    "extract_folder = \"extracted_files/\"  # Where CSVs will be extracted\n",
    "\n",
    "# Ensure the extraction folder exists\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "# Find all ZIP files in the zip_files folder\n",
    "zip_files = glob.glob(os.path.join(zip_folder, \"*.zip\"))\n",
    "\n",
    "if not zip_files:\n",
    "    print(\"❌ No ZIP files found in 'csv_files/'. Please check the folder.\")\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "            # Extract only CSV files to the extracted_files folder\n",
    "            for file in zip_ref.namelist():\n",
    "                if file.endswith(\".csv\"):  # Only extract CSV files\n",
    "                    zip_ref.extract(file, extract_folder)\n",
    "                    print(f\"✅ Extracted: {file} from {zip_file} to {extract_folder}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting {zip_file}: {e}\")\n",
    "\n",
    "# Check if any CSV files exist in extracted_files/\n",
    "csv_files = glob.glob(os.path.join(extract_folder, \"*.csv\"))\n",
    "if csv_files:\n",
    "    print(f\"✅ Successfully extracted {len(csv_files)} CSV files to '{extract_folder}'\")\n",
    "else:\n",
    "    print(\"⚠️ No CSV files found after extraction. Verify ZIP contents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load and combine all extracted CSV files\n",
    "csv_files = glob.glob(os.path.join(extract_folder, \"*.csv\"))\n",
    "\n",
    "all_data_frames = []\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"❌ No CSV files found in 'extracted_files/'. Ensure ZIPs contain CSVs.\")\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding=\"utf-8-sig\")  # Ensure correct encoding\n",
    "        if df.empty:\n",
    "            print(f\"⚠️ Warning: {file} is empty and was skipped.\")\n",
    "        else:\n",
    "            print(f\"✅ Loaded {file} ({df.shape[0]} rows, {df.shape[1]} columns).\")\n",
    "            all_data_frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file}: {e}\")\n",
    "\n",
    "# Merge all CSVs into a single DataFrame\n",
    "if all_data_frames:\n",
    "    combined_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "    print(f\"✅ Merged {len(all_data_frames)} files into `combined_df` ({combined_df.shape[0]} rows).\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "    print(\"⚠️ No valid data loaded. Check extracted CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "zip_folder = \"csv_files/\"  # Where ZIP files are located\n",
    "extract_folder = \"extracted_files/\"  # Where CSVs will be extracted\n",
    "\n",
    "# Ensure the extraction folder exists\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "# Find all ZIP files in the zip_files folder\n",
    "zip_files = glob.glob(os.path.join(zip_folder, \"*.zip\"))\n",
    "\n",
    "if not zip_files:\n",
    "    print(\"❌ No ZIP files found in 'csv_files/'. Please check the folder.\")\n",
    "\n",
    "for zip_file in zip_files:\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "            # Extract only CSV files to the extracted_files folder\n",
    "            for file in zip_ref.namelist():\n",
    "                if file.endswith(\".csv\"):  # Only extract CSV files\n",
    "                    zip_ref.extract(file, extract_folder)\n",
    "                    print(f\"✅ Extracted: {file} from {zip_file} to {extract_folder}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting {zip_file}: {e}\")\n",
    "\n",
    "# Check if any CSV files exist in extracted_files/\n",
    "csv_files = glob.glob(os.path.join(extract_folder, \"*.csv\"))\n",
    "if csv_files:\n",
    "    print(f\"✅ Successfully extracted {len(csv_files)} CSV files to '{extract_folder}'\")\n",
    "else:\n",
    "    print(\"⚠️ No CSV files found after extraction. Verify ZIP contents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load and combine all extracted CSV files\n",
    "csv_files = glob.glob(os.path.join(extract_folder, \"*.csv\"))\n",
    "\n",
    "all_data_frames = []\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"❌ No CSV files found in 'extracted_files/'. Ensure ZIPs contain CSVs.\")\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding=\"utf-8-sig\")  # Ensure correct encoding\n",
    "        if df.empty:\n",
    "            print(f\"⚠️ Warning: {file} is empty and was skipped.\")\n",
    "        else:\n",
    "            print(f\"✅ Loaded {file} ({df.shape[0]} rows, {df.shape[1]} columns).\")\n",
    "            all_data_frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file}: {e}\")\n",
    "\n",
    "# Merge all CSVs into a single DataFrame\n",
    "if all_data_frames:\n",
    "    combined_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "    print(f\"✅ Merged {len(all_data_frames)} files into `combined_df` ({combined_df.shape[0]} rows).\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "    print(\"⚠️ No valid data loaded. Check extracted CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    # Standardize column names (lowercase, replace spaces with underscores)\n",
    "    combined_df.columns = (\n",
    "        combined_df.columns.str.strip().str.lower().str.replace(\" \", \"_\", regex=True)\n",
    "    )\n",
    "\n",
    "    # Convert 'date' column to datetime format\n",
    "    combined_df[\"date\"] = pd.to_datetime(combined_df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with missing dates\n",
    "    combined_df = combined_df.dropna(subset=[\"date\"])\n",
    "\n",
    "    print(\"✅ Column names standardized and date column processed.\")\n",
    "else:\n",
    "    print(\"⚠️ `combined_df` is empty. Investigate ZIP or CSV issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define valid columns that can be used for graphing\n",
    "valid_columns = [\n",
    "    \"time_to_takeoff\", \"mrsi\", \"jump_height\", \"braking_rfd\", \n",
    "    \"countermovement_depth\", \"peak_velocity\", \"braking_phase\", \n",
    "    \"flight_time\", \"takeoff_velocity\", \"peak_braking_velocity\", \n",
    "    \"propulsive_phase\"\n",
    "]\n",
    "\n",
    "# Keep only columns that exist in the dataset\n",
    "valid_columns = [col for col in valid_columns if col in combined_df.columns]\n",
    "\n",
    "print(f\"✅ Available numeric columns: {valid_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dropdown options for athlete selection\n",
    "athletes = sorted(combined_df[\"name\"].unique())\n",
    "\n",
    "# Date pickers for filtering dataset\n",
    "min_date = combined_df[\"date\"].min()\n",
    "max_date = combined_df[\"date\"].max()\n",
    "\n",
    "start_date_picker = DatePicker(\n",
    "    description=\"Start Date:\",\n",
    "    value=min_date,\n",
    "    layout=Layout(width='40%')\n",
    ")\n",
    "\n",
    "end_date_picker = DatePicker(\n",
    "    description=\"End Date:\",\n",
    "    value=max_date,\n",
    "    layout=Layout(width='40%')\n",
    ")\n",
    "\n",
    "# Dropdown for athlete selection\n",
    "athlete_dropdown = Dropdown(\n",
    "    options=athletes,\n",
    "    description=\"Athlete:\",\n",
    "    layout=Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Dropdowns for selecting variables\n",
    "var1_dropdown = Dropdown(\n",
    "    options=valid_columns,\n",
    "    value=valid_columns[0],\n",
    "    description=\"Variable 1:\",\n",
    "    layout=Layout(width='50%')\n",
    ")\n",
    "\n",
    "var2_dropdown = Dropdown(\n",
    "    options=valid_columns,\n",
    "    value=valid_columns[1],\n",
    "    description=\"Variable 2:\",\n",
    "    layout=Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Output widget for displaying the graph\n",
    "output = Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to process the data (filter top 2 jumps and average them)\n",
    "def process_top_jumps(data):\n",
    "    if data.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Get top 2 jumps per day\n",
    "    top_jumps = (\n",
    "        data.sort_values(by=\"jump_height\", ascending=False)\n",
    "        .groupby(\"date\")\n",
    "        .head(2)\n",
    "    )\n",
    "\n",
    "    # Compute the mean for numeric columns\n",
    "    processed_data = top_jumps.groupby(\"date\", as_index=False).mean(numeric_only=True)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to update the Plotly graph with smooth curves & shading\n",
    "def update_graph(athlete, var1, var2, start_date, end_date):\n",
    "    with output:\n",
    "        output.clear_output(wait=True)\n",
    "\n",
    "        # Convert date selection to datetime\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "        # Filter data for the selected athlete and date range\n",
    "        athlete_data = combined_df[\n",
    "            (combined_df[\"name\"] == athlete) & \n",
    "            (combined_df[\"date\"] >= start_date) & \n",
    "            (combined_df[\"date\"] <= end_date)\n",
    "        ]\n",
    "\n",
    "        if athlete_data.empty:\n",
    "            print(\"❌ No data available for the selected range.\")\n",
    "            return\n",
    "\n",
    "        # Process the top 2 jumps per day\n",
    "        athlete_data = process_top_jumps(athlete_data)\n",
    "\n",
    "        if athlete_data.empty:\n",
    "            print(\"❌ No valid data after processing top 2 jumps per day.\")\n",
    "            return\n",
    "\n",
    "        # Compute mean and standard deviation for scaling\n",
    "        var1_mean = athlete_data[var1].mean()\n",
    "        var1_std = athlete_data[var1].std()\n",
    "        var2_mean = athlete_data[var2].mean()\n",
    "        var2_std = athlete_data[var2].std()\n",
    "\n",
    "        # Set y-axis ranges independently\n",
    "        var1_min = max(0, var1_mean - 2 * var1_std)\n",
    "        var1_max = athlete_data[var1].max() + 0.01\n",
    "        var2_min = max(0, var2_mean - 2 * var2_std)\n",
    "        var2_max = athlete_data[var2].max() + 0.01\n",
    "\n",
    "        # Create the figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add trace for Variable 1 (Left y-axis) - RED\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=athlete_data[\"date\"],\n",
    "            y=athlete_data[var1],\n",
    "            mode='lines+markers',\n",
    "            name=f\"{var1}\",\n",
    "            line=dict(color='red', shape='spline', width=3),\n",
    "            fill='tonexty',\n",
    "            fillcolor='rgba(255, 0, 0, 0.3)'  # Red shading\n",
    "        ))\n",
    "\n",
    "        # Add trace for Variable 2 (Right y-axis) - YELLOW\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=athlete_data[\"date\"],\n",
    "            y=athlete_data[var2],\n",
    "            mode='lines+markers',\n",
    "            name=f\"{var2}\",\n",
    "            line=dict(color='yellow', shape='spline', width=3),\n",
    "            fill='tonexty',\n",
    "            fillcolor='rgba(255, 255, 0, 0.3)',  # Yellow shading\n",
    "            yaxis=\"y2\"\n",
    "        ))\n",
    "\n",
    "        # Set layout with black background and red/yellow styling\n",
    "        fig.update_layout(\n",
    "            title=f\"Performance Over Time - {athlete}\",\n",
    "            xaxis=dict(title=\"Date\", tickformat=\"%m-%d\", gridcolor=\"gray\", color=\"white\"),\n",
    "            yaxis=dict(\n",
    "                title=f\"{var1}\",\n",
    "                titlefont=dict(color=\"red\"),\n",
    "                tickfont=dict(color=\"red\"),\n",
    "                gridcolor=\"gray\",\n",
    "                range=[var1_min, var1_max]  \n",
    "            ),\n",
    "            yaxis2=dict(\n",
    "                title=f\"{var2}\",\n",
    "                titlefont=dict(color=\"yellow\"),  \n",
    "                tickfont=dict(color=\"yellow\"),  \n",
    "                overlaying=\"y\",\n",
    "                side=\"right\",\n",
    "                gridcolor=\"gray\",\n",
    "                range=[var2_min, var2_max]  \n",
    "            ),\n",
    "            paper_bgcolor=\"black\",  \n",
    "            plot_bgcolor=\"black\",  \n",
    "            template=\"plotly_dark\",\n",
    "            showlegend=True\n",
    "        )\n",
    "\n",
    "        # Show the graph\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "interact(\n",
    "    update_graph,\n",
    "    athlete=athlete_dropdown,\n",
    "    var1=var1_dropdown,\n",
    "    var2=var2_dropdown,\n",
    "    start_date=start_date_picker,\n",
    "    end_date=end_date_picker\n",
    ")\n",
    "\n",
    "display(output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
